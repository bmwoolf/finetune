{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Learning the Language of Proteins - Streamlined\n",
        "\n",
        "Essential code for protein function prediction using ESM2 embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install dependencies and import required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch transformers jax flax optax tensorflow pandas scikit-learn matplotlib seaborn tqdm pyarrow obonet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from transformers import AutoTokenizer, EsmModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from flax.training.train_state import TrainState\n",
        "from Bio import SeqIO\n",
        "import os\n",
        "import math\n",
        "from typing import Dict, List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Essential dlfb Functions\n",
        "\n",
        "Core functions needed for the protein function prediction pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assets(subdir: str = None) -> str:\n",
        "    \"\"\"Get path to assets directory.\"\"\"\n",
        "    assets_dir = \"./data\"\n",
        "    if subdir:\n",
        "        assets_dir = os.path.join(assets_dir, subdir)\n",
        "    return assets_dir\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Get available device (GPU or CPU).\"\"\"\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_go_term_descriptions(store_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Return GO term to description mapping, downloading if needed.\"\"\"\n",
        "    if not os.path.exists(store_path):\n",
        "        import obonet\n",
        "        url = \"https://current.geneontology.org/ontology/go-basic.obo\"\n",
        "        graph = obonet.read_obo(url)\n",
        "        \n",
        "        # Extract GO term IDs and names from the graph nodes.\n",
        "        id_to_name = {id: data.get(\"name\") for id, data in graph.nodes(data=True)}\n",
        "        go_term_descriptions = pd.DataFrame(\n",
        "            zip(id_to_name.keys(), id_to_name.values()),\n",
        "            columns=[\"term\", \"description\"],\n",
        "        )\n",
        "        go_term_descriptions.to_csv(store_path, index=False)\n",
        "    else:\n",
        "        go_term_descriptions = pd.read_csv(store_path)\n",
        "    return go_term_descriptions\n",
        "\n",
        "def get_mean_embeddings(\n",
        "    sequences: List[str],\n",
        "    tokenizer,\n",
        "    model,\n",
        "    device: torch.device = None,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Compute mean embedding for each sequence using a protein LM.\"\"\"\n",
        "    if not device:\n",
        "        device = get_device()\n",
        "\n",
        "    # Tokenize input sequences and pad them to equal length.\n",
        "    model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Move tokenized inputs to the target device (CPU or GPU).\n",
        "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "    # Move model to the target device and set it to evaluation mode.\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Forward pass without gradient tracking to obtain embeddings.\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**model_inputs)\n",
        "        mean_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    return mean_embeddings.detach().cpu().numpy()\n",
        "\n",
        "def store_sequence_embeddings(\n",
        "    sequence_df: pd.DataFrame,\n",
        "    store_prefix: str,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    batch_size: int = 64,\n",
        "    force: bool = False,\n",
        ") -> None:\n",
        "    \"\"\"Extract and store mean embeddings for each protein sequence.\"\"\"\n",
        "    model_name = str(model.name_or_path).replace(\"/\", \"_\")\n",
        "    store_file = f\"{store_prefix}_{model_name}.feather\"\n",
        "\n",
        "    if not os.path.exists(store_file) or force:\n",
        "        device = get_device()\n",
        "\n",
        "        # Iterate through protein dataframe in batches, extracting embeddings.\n",
        "        n_batches = math.ceil(sequence_df.shape[0] / batch_size)\n",
        "        batches: List[np.ndarray] = []\n",
        "        for i in range(n_batches):\n",
        "            batch_seqs = list(\n",
        "                sequence_df[\"Sequence\"][i * batch_size : (i + 1) * batch_size]\n",
        "            )\n",
        "            batches.extend(get_mean_embeddings(batch_seqs, tokenizer, model, device))\n",
        "\n",
        "        # Store each of the embedding values in a separate column in the dataframe.\n",
        "        embeddings = pd.DataFrame(np.vstack(batches))\n",
        "        embeddings.columns = [f\"ME:{int(i)+1}\" for i in range(embeddings.shape[1])]\n",
        "        df = pd.concat([sequence_df.reset_index(drop=True), embeddings], axis=1)\n",
        "        df.to_feather(store_file)\n",
        "\n",
        "def load_sequence_embeddings(\n",
        "    store_file_prefix: str, model_checkpoint: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Load stored embedding DataFrame from disk.\"\"\"\n",
        "    model_name = model_checkpoint.replace(\"/\", \"_\")\n",
        "    store_file = f\"{store_file_prefix}_{model_name}.feather\"\n",
        "    return pd.read_feather(store_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"Simple MLP for protein function prediction.\"\"\"\n",
        "    \n",
        "    num_targets: int\n",
        "    dim: int = 256\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Apply MLP layers to input features.\"\"\"\n",
        "        x = nn.Sequential([\n",
        "            nn.Dense(self.dim * 2),\n",
        "            jax.nn.gelu,\n",
        "            nn.Dense(self.dim),\n",
        "            jax.nn.gelu,\n",
        "            nn.Dense(self.num_targets),\n",
        "        ])(x)\n",
        "        return x\n",
        "\n",
        "    def create_train_state(self, rng: jax.Array, dummy_input, tx) -> TrainState:\n",
        "        \"\"\"Initialize model parameters and return a training state.\"\"\"\n",
        "        variables = self.init(rng, dummy_input)\n",
        "        return TrainState.create(\n",
        "            apply_fn=self.apply, params=variables[\"params\"], tx=tx\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_tfds(\n",
        "    df: pd.DataFrame,\n",
        "    embeddings_prefix: str = \"ME:\",\n",
        "    target_prefix: str = \"GO:\",\n",
        "    is_training: bool = False,\n",
        "    shuffle_buffer: int = 50,\n",
        ") -> tf.data.Dataset:\n",
        "    \"\"\"Convert embedding DataFrame into a TensorFlow dataset.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\n",
        "        \"embedding\": df.filter(regex=f\"^{embeddings_prefix}\").to_numpy(),\n",
        "        \"target\": df.filter(regex=f\"^{target_prefix}\").to_numpy(),\n",
        "    })\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(shuffle_buffer).repeat()\n",
        "    return dataset\n",
        "\n",
        "def compute_metrics(\n",
        "    targets: np.ndarray, probs: np.ndarray, thresh=0.5\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Compute accuracy, recall, precision, auPRC, and auROC.\"\"\"\n",
        "    if np.sum(targets) == 0:\n",
        "        return {\n",
        "            m: 0.0 for m in [\"accuracy\", \"recall\", \"precision\", \"auprc\", \"auroc\"]\n",
        "        }\n",
        "    return {\n",
        "        \"accuracy\": metrics.accuracy_score(targets, probs >= thresh),\n",
        "        \"recall\": metrics.recall_score(targets, probs >= thresh).item(),\n",
        "        \"precision\": metrics.precision_score(\n",
        "            targets,\n",
        "            probs >= thresh,\n",
        "            zero_division=0.0,\n",
        "        ).item(),\n",
        "        \"auprc\": metrics.average_precision_score(targets, probs).item(),\n",
        "        \"auroc\": metrics.roc_auc_score(targets, probs).item(),\n",
        "    }\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, batch):\n",
        "    \"\"\"Run a single training step and update model parameters.\"\"\"\n",
        "    \n",
        "    def calculate_loss(params):\n",
        "        \"\"\"Compute sigmoid cross-entropy loss from logits.\"\"\"\n",
        "        logits = state.apply_fn({\"params\": params}, x=batch[\"embedding\"])\n",
        "        loss = optax.sigmoid_binary_cross_entropy(logits, batch[\"target\"]).mean()\n",
        "        return loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(calculate_loss, has_aux=False)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "def eval_step(state, batch) -> Dict[str, float]:\n",
        "    \"\"\"Run evaluation step and return mean metrics over targets.\"\"\"\n",
        "    logits = state.apply_fn({\"params\": state.params}, x=batch[\"embedding\"])\n",
        "    loss = optax.sigmoid_binary_cross_entropy(logits, batch[\"target\"]).mean()\n",
        "    \n",
        "    # Calculate per-target metrics\n",
        "    probs = jax.nn.sigmoid(logits)\n",
        "    target_metrics = []\n",
        "    for target, prob in zip(batch[\"target\"], probs):\n",
        "        target_metrics.append(compute_metrics(target, prob))\n",
        "    \n",
        "    metrics_dict = {\n",
        "        \"loss\": loss.item(),\n",
        "        **pd.DataFrame(target_metrics).mean(axis=0).to_dict(),\n",
        "    }\n",
        "    return metrics_dict\n",
        "\n",
        "def train(\n",
        "    state: TrainState,\n",
        "    dataset_splits: Dict[str, tf.data.Dataset],\n",
        "    batch_size: int,\n",
        "    num_steps: int = 300,\n",
        "    eval_every: int = 30,\n",
        "):\n",
        "    \"\"\"Train model using batched TF datasets and track performance metrics.\"\"\"\n",
        "    # Create containers to handle calculated during training and evaluation.\n",
        "    train_metrics, valid_metrics = [], []\n",
        "\n",
        "    # Create batched dataset to pluck batches from for each step.\n",
        "    train_batches = (\n",
        "        dataset_splits[\"train\"]\n",
        "        .batch(batch_size, drop_remainder=True)\n",
        "        .as_numpy_iterator()\n",
        "    )\n",
        "\n",
        "    steps = tqdm(range(num_steps))  # Steps with progress bar.\n",
        "    for step in steps:\n",
        "        steps.set_description(f\"Step {step + 1}\")\n",
        "\n",
        "        # Get batch of training data, convert into a JAX array, and train.\n",
        "        state, loss = train_step(state, next(train_batches))\n",
        "        train_metrics.append({\"step\": step, \"loss\": loss.item()})\n",
        "\n",
        "        if step % eval_every == 0:\n",
        "            # For all the evaluation batches, calculate metrics.\n",
        "            eval_metrics = []\n",
        "            for eval_batch in (\n",
        "                dataset_splits[\"valid\"].batch(batch_size=batch_size).as_numpy_iterator()\n",
        "            ):\n",
        "                eval_metrics.append(eval_step(state, eval_batch))\n",
        "            valid_metrics.append(\n",
        "                {\"step\": step, **pd.DataFrame(eval_metrics).mean(axis=0).to_dict()}\n",
        "            )\n",
        "\n",
        "    return state, {\"train\": train_metrics, \"valid\": valid_metrics}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation & Training\n",
        "\n",
        "### Create dummy data and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dummy CAFA3-like data for demonstration\n",
        "print(\"Creating dummy CAFA3-like data...\")\n",
        "\n",
        "# Create dummy labels dataframe\n",
        "n_proteins = 1000\n",
        "n_go_terms = 50\n",
        "\n",
        "dummy_labels = []\n",
        "for protein_id in range(n_proteins):\n",
        "    # Each protein gets 2-5 random GO terms\n",
        "    n_terms = np.random.randint(2, 6)\n",
        "    terms = np.random.choice(n_go_terms, n_terms, replace=False)\n",
        "    for term in terms:\n",
        "        dummy_labels.append({\n",
        "            'EntryID': f'PROTEIN_{protein_id:06d}',\n",
        "            'term': f'GO:{term:07d}',\n",
        "            'aspect': 'MFO'\n",
        "        })\n",
        "\n",
        "labels = pd.DataFrame(dummy_labels)\n",
        "print(f\"Created {len(labels)} protein-function annotations\")\n",
        "\n",
        "# Create dummy GO term descriptions\n",
        "go_descriptions = []\n",
        "for i in range(n_go_terms):\n",
        "    go_descriptions.append({\n",
        "        'term': f'GO:{i:07d}',\n",
        "        'description': f'Molecular function {i}'\n",
        "    })\n",
        "\n",
        "go_term_descriptions = pd.DataFrame(go_descriptions)\n",
        "os.makedirs(assets(), exist_ok=True)\n",
        "go_term_descriptions.to_csv(assets(\"go_term_descriptions.csv\"), index=False)\n",
        "\n",
        "# Merge labels with descriptions\n",
        "labels = labels.merge(go_term_descriptions, on=\"term\")\n",
        "print(f\"Labels with descriptions: {len(labels)} rows\")\n",
        "\n",
        "# Create dummy protein sequences\n",
        "amino_acids = list(\"ARNDCQEGHILKMFPSTWYV\")\n",
        "sequences = []\n",
        "for protein_id in range(n_proteins):\n",
        "    # Random sequence length between 50-500\n",
        "    seq_len = np.random.randint(50, 500)\n",
        "    sequence = ''.join(np.random.choice(amino_acids, seq_len))\n",
        "    sequences.append({\n",
        "        'EntryID': f'PROTEIN_{protein_id:06d}',\n",
        "        'Sequence': sequence,\n",
        "        'Length': seq_len,\n",
        "        'taxonomyID': 9606  # Human\n",
        "    })\n",
        "\n",
        "sequence_df = pd.DataFrame(sequences)\n",
        "\n",
        "# Merge with taxonomy and labels\n",
        "sequence_df = sequence_df.merge(labels, on=\"EntryID\")\n",
        "print(f\"Final dataset: {sequence_df['EntryID'].nunique()} proteins with {sequence_df['term'].nunique()} functions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for common functions (appears in at least 10 proteins)\n",
        "common_functions = (\n",
        "    sequence_df[\"term\"]\n",
        "    .value_counts()[sequence_df[\"term\"].value_counts() >= 10]\n",
        "    .index\n",
        ")\n",
        "sequence_df = sequence_df[sequence_df[\"term\"].isin(common_functions)]\n",
        "print(f\"After filtering: {sequence_df['EntryID'].nunique()} proteins with {sequence_df['term'].nunique()} functions\")\n",
        "\n",
        "# Convert to multi-label format\n",
        "sequence_df = (\n",
        "    sequence_df[[\"EntryID\", \"Sequence\", \"Length\", \"term\"]]\n",
        "    .assign(value=1)\n",
        "    .pivot(\n",
        "        index=[\"EntryID\", \"Sequence\", \"Length\"], columns=\"term\", values=\"value\"\n",
        "    )\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        "    .reset_index()\n",
        ")\n",
        "print(f\"Multi-label format: {sequence_df.shape}\")\n",
        "\n",
        "# Filter by length\n",
        "sequence_df = sequence_df[sequence_df[\"Length\"] <= 500]\n",
        "print(f\"After length filter: {sequence_df.shape}\")\n",
        "\n",
        "# Split the dataset\n",
        "train_sequence_ids, valid_test_sequence_ids = train_test_split(\n",
        "    list(set(sequence_df[\"EntryID\"])), test_size=0.40, random_state=42\n",
        ")\n",
        "valid_sequence_ids, test_sequence_ids = train_test_split(\n",
        "    valid_test_sequence_ids, test_size=0.50, random_state=42\n",
        ")\n",
        "\n",
        "sequence_splits = {\n",
        "    \"train\": sequence_df[sequence_df[\"EntryID\"].isin(train_sequence_ids)],\n",
        "    \"valid\": sequence_df[sequence_df[\"EntryID\"].isin(valid_sequence_ids)],\n",
        "    \"test\": sequence_df[sequence_df[\"EntryID\"].isin(test_sequence_ids)],\n",
        "}\n",
        "\n",
        "for split, df in sequence_splits.items():\n",
        "    print(f\"{split} has {len(df)} entries.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ESM2 model and extract embeddings\n",
        "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
        "print(f\"Loading ESM2 model: {model_checkpoint}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = EsmModel.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Store embeddings for each split\n",
        "for split, df in sequence_splits.items():\n",
        "    print(f\"Processing {split} split...\")\n",
        "    store_sequence_embeddings(\n",
        "        sequence_df=df,\n",
        "        store_prefix=assets(f\"protein_dataset_{split}\"),\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "    )\n",
        "\n",
        "# Load the training data back\n",
        "train_df = load_sequence_embeddings(\n",
        "    assets(\"protein_dataset_train\"),\n",
        "    model_checkpoint=model_checkpoint,\n",
        ")\n",
        "print(f\"Loaded training data: {train_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build dataset splits and initialize model\n",
        "dataset_splits = {}\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    dataset_splits[split] = convert_to_tfds(\n",
        "        df=load_sequence_embeddings(\n",
        "            store_file_prefix=f\"{assets('protein_dataset')}_{split}\",\n",
        "            model_checkpoint=model_checkpoint,\n",
        "        ),\n",
        "        is_training=(split == \"train\"),\n",
        "    )\n",
        "\n",
        "# Get a batch to initialize the model\n",
        "batch_size = 32\n",
        "batch = next(dataset_splits[\"train\"].batch(batch_size).as_numpy_iterator())\n",
        "print(f\"Batch shapes: embedding={batch['embedding'].shape}, target={batch['target'].shape}\")\n",
        "\n",
        "# Initialize model\n",
        "targets = list(train_df.columns[train_df.columns.str.contains(\"GO:\")])\n",
        "print(f\"Number of target functions: {len(targets)}\")\n",
        "\n",
        "mlp = Model(num_targets=len(targets))\n",
        "\n",
        "# Initialize training state\n",
        "rng = jax.random.PRNGKey(42)\n",
        "rng, rng_init = jax.random.split(key=rng, num=2)\n",
        "\n",
        "state = mlp.create_train_state(\n",
        "    rng=rng_init, \n",
        "    dummy_input=batch[\"embedding\"], \n",
        "    tx=optax.adam(0.001)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "state, metrics = train(\n",
        "    state=state,\n",
        "    dataset_splits=dataset_splits,\n",
        "    batch_size=batch_size,\n",
        "    num_steps=300,\n",
        "    eval_every=30,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "valid_df = load_sequence_embeddings(\n",
        "    store_file_prefix=f\"{assets('protein_dataset')}_valid\",\n",
        "    model_checkpoint=model_checkpoint,\n",
        ")\n",
        "\n",
        "# Generate predictions\n",
        "valid_probs = []\n",
        "for valid_batch in dataset_splits[\"valid\"].batch(1).as_numpy_iterator():\n",
        "    logits = state.apply_fn({\"params\": state.params}, x=valid_batch[\"embedding\"])\n",
        "    valid_probs.extend(jax.nn.sigmoid(logits))\n",
        "\n",
        "valid_true_df = valid_df[[\"EntryID\"] + targets].set_index(\"EntryID\")\n",
        "valid_prob_df = pd.DataFrame(\n",
        "    np.stack(valid_probs), columns=targets, index=valid_true_df.index\n",
        ")\n",
        "\n",
        "print(f\"Validation predictions shape: {valid_prob_df.shape}\")\n",
        "\n",
        "# Calculate metrics by function\n",
        "metrics_by_function = {}\n",
        "for function in targets:\n",
        "    metrics_by_function[function] = compute_metrics(\n",
        "        valid_true_df[function].values, valid_prob_df[function].values\n",
        "    )\n",
        "\n",
        "overview_valid = (\n",
        "    pd.DataFrame(metrics_by_function)\n",
        "    .T.merge(go_term_descriptions, left_index=True, right_on=\"term\")\n",
        "    .set_index(\"term\")\n",
        "    .sort_values(\"auprc\", ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\nTop 10 performing functions:\")\n",
        "print(overview_valid.head(10)[[\"description\", \"auprc\", \"auroc\"]])\n",
        "\n",
        "# Final check on test set\n",
        "eval_metrics = []\n",
        "for split in [\"valid\", \"test\"]:\n",
        "    split_metrics = []\n",
        "    for eval_batch in dataset_splits[split].batch(32).as_numpy_iterator():\n",
        "        split_metrics.append(eval_step(state, eval_batch))\n",
        "    eval_metrics.append(\n",
        "        {\"split\": split, **pd.DataFrame(split_metrics).mean(axis=0).to_dict()}\n",
        "    )\n",
        "\n",
        "final_results = pd.DataFrame(eval_metrics)\n",
        "print(\"\\nFinal Results:\")\n",
        "print(final_results)\n",
        "\n",
        "print(\"\\n=== Chapter 2 Complete ===\")\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"Final validation AUPRC: {final_results[final_results['split'] == 'valid']['auprc'].iloc[0]:.4f}\")\n",
        "print(f\"Final test AUPRC: {final_results[final_results['split'] == 'test']['auprc'].iloc[0]:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
